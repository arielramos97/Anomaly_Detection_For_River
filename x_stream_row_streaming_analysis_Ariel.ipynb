{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# xStream (row streaming) vs static results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "import tqdm\n",
    "from XStream_River import xStream"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = gzip.open(\"data/cancer.gz\", \"r\")\n",
    "\n",
    "X, y = [], []\n",
    "\n",
    "for i in data:\n",
    "  i = (i.decode('utf-8')).split(\",\")\n",
    "  i = [float(x) for x in i]\n",
    "  X.append(np.array(i[:-1]))\n",
    "  #X.append(i[:-1])\n",
    "  y.append(i[-1])\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 385/385 [00:20<00:00, 18.83it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Only one class present in y_true. ROC AUC score is not defined in that case.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 38\u001b[0m\n\u001b[0;32m     33\u001b[0m         all_scores\u001b[39m.\u001b[39mappend(anomaly_score[\u001b[39m0\u001b[39m])\n\u001b[0;32m     35\u001b[0m     \u001b[39m# model.learn_one(x)\u001b[39;00m\n\u001b[0;32m     36\u001b[0m     \u001b[39m# model.fit_partial(x)\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mauc is \u001b[39m\u001b[39m{\u001b[39;00mmetric\u001b[39m.\u001b[39;49mget()\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     41\u001b[0m \u001b[39m# y_adjusted = y[window_size:]\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[39m# print(average_precision_score(y_adjusted, all_scores))\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[39m#     \"MAP =\", MAP, \"\\n\\t \", \u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[39m#     \"AUC =\", AUC)\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\aramosvela\\Documents\\Data_Science\\uni_env\\lib\\site-packages\\pysad\\evaluation\\metrics.py:30\u001b[0m, in \u001b[0;36mBaseSKLearnMetric.get\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m     25\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Gets the current value of the score.\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \n\u001b[0;32m     27\u001b[0m \u001b[39m    Returns:\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[39m        float: The current score.\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m     score \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluate(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49my_true, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49my_pred)\n\u001b[0;32m     32\u001b[0m     \u001b[39mreturn\u001b[39;00m score\n",
      "File \u001b[1;32mc:\\Users\\aramosvela\\Documents\\Data_Science\\uni_env\\lib\\site-packages\\pysad\\evaluation\\metrics.py:66\u001b[0m, in \u001b[0;36mAUROCMetric._evaluate\u001b[1;34m(self, y_true, y_pred)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_evaluate\u001b[39m(\u001b[39mself\u001b[39m, y_true, y_pred):\n\u001b[1;32m---> 66\u001b[0m     \u001b[39mreturn\u001b[39;00m roc_auc_score(y_true, y_pred)\n",
      "File \u001b[1;32mc:\\Users\\aramosvela\\Documents\\Data_Science\\uni_env\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:580\u001b[0m, in \u001b[0;36mroc_auc_score\u001b[1;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[0;32m    572\u001b[0m     \u001b[39mreturn\u001b[39;00m _average_binary_score(\n\u001b[0;32m    573\u001b[0m         partial(_binary_roc_auc_score, max_fpr\u001b[39m=\u001b[39mmax_fpr),\n\u001b[0;32m    574\u001b[0m         y_true,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    577\u001b[0m         sample_weight\u001b[39m=\u001b[39msample_weight,\n\u001b[0;32m    578\u001b[0m     )\n\u001b[0;32m    579\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# multilabel-indicator\u001b[39;00m\n\u001b[1;32m--> 580\u001b[0m     \u001b[39mreturn\u001b[39;00m _average_binary_score(\n\u001b[0;32m    581\u001b[0m         partial(_binary_roc_auc_score, max_fpr\u001b[39m=\u001b[39;49mmax_fpr),\n\u001b[0;32m    582\u001b[0m         y_true,\n\u001b[0;32m    583\u001b[0m         y_score,\n\u001b[0;32m    584\u001b[0m         average,\n\u001b[0;32m    585\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m    586\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\aramosvela\\Documents\\Data_Science\\uni_env\\lib\\site-packages\\sklearn\\metrics\\_base.py:118\u001b[0m, in \u001b[0;36m_average_binary_score\u001b[1;34m(binary_metric, y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[0;32m    116\u001b[0m     y_true_c \u001b[39m=\u001b[39m y_true\u001b[39m.\u001b[39mtake([c], axis\u001b[39m=\u001b[39mnot_average_axis)\u001b[39m.\u001b[39mravel()\n\u001b[0;32m    117\u001b[0m     y_score_c \u001b[39m=\u001b[39m y_score\u001b[39m.\u001b[39mtake([c], axis\u001b[39m=\u001b[39mnot_average_axis)\u001b[39m.\u001b[39mravel()\n\u001b[1;32m--> 118\u001b[0m     score[c] \u001b[39m=\u001b[39m binary_metric(y_true_c, y_score_c, sample_weight\u001b[39m=\u001b[39;49mscore_weight)\n\u001b[0;32m    120\u001b[0m \u001b[39m# Average the results\u001b[39;00m\n\u001b[0;32m    121\u001b[0m \u001b[39mif\u001b[39;00m average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\aramosvela\\Documents\\Data_Science\\uni_env\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:339\u001b[0m, in \u001b[0;36m_binary_roc_auc_score\u001b[1;34m(y_true, y_score, sample_weight, max_fpr)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Binary roc auc score.\"\"\"\u001b[39;00m\n\u001b[0;32m    338\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(np\u001b[39m.\u001b[39munique(y_true)) \u001b[39m!=\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m--> 339\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    340\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mOnly one class present in y_true. ROC AUC score \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    341\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mis not defined in that case.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    342\u001b[0m     )\n\u001b[0;32m    344\u001b[0m fpr, tpr, _ \u001b[39m=\u001b[39m roc_curve(y_true, y_score, sample_weight\u001b[39m=\u001b[39msample_weight)\n\u001b[0;32m    345\u001b[0m \u001b[39mif\u001b[39;00m max_fpr \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m max_fpr \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n",
      "\u001b[1;31mValueError\u001b[0m: Only one class present in y_true. ROC AUC score is not defined in that case."
     ]
    }
   ],
   "source": [
    "#Row streaming results\n",
    "\n",
    "#Check if same performance using pysad=\n",
    "from pysad.models import xStream as xStream2\n",
    "from pysad.evaluation.metrics import AUPRMetric, AUROCMetric\n",
    "\n",
    "k = 100 \n",
    "n_chains = 100\n",
    "depth = 15\n",
    "window_size = int(0.10*len(y))\n",
    "# window_size = 25\n",
    "\n",
    "metric = AUROCMetric()\n",
    "\n",
    "for i in range(10): # Change the number of runs (depending if the dataset is big)\n",
    "\n",
    "    #I think it does not make sense to shuffle the data\n",
    "    # X_shuffled, y_shuffled = shuffle(X, y, random_state=i)\n",
    "    \n",
    "    model = xStream2(num_components=k, n_chains=n_chains, depth=depth, window_size=window_size) \n",
    "\n",
    "    all_scores = []\n",
    "    for i, x in enumerate(tqdm.tqdm(X)):\n",
    "       \n",
    "        # anomalyscore = -model.predict_one(x)\n",
    "        # anomalyscore = -model.learn_one(x)\n",
    "\n",
    "        anomaly_score = model.score_partial(x)\n",
    "        model = model.fit_partial(x)\n",
    "        metric.update(y[i], anomaly_score)\n",
    "\n",
    "        if i>=window_size:\n",
    "            all_scores.append(anomaly_score[0])\n",
    "        \n",
    "        # model.learn_one(x)\n",
    "        # model.fit_partial(x)\n",
    "    \n",
    "    print(f\"auc is {metric.get()}.\")\n",
    "    \n",
    "\n",
    "    # y_adjusted = y[window_size:]\n",
    "    # print(average_precision_score(y_adjusted, all_scores))\n",
    "\n",
    "    # chunks = [all_scores[j:j+window_size] for j in range(0, len(all_scores), window_size)]\n",
    "    # y_chunks = [y_adjusted[j:j+window_size] for j in range(0, len(y_adjusted), window_size)]\n",
    "\n",
    "    # AP_window = []\n",
    "    # for i in range(len(y_chunks)-1):\n",
    "    #     score = average_precision_score(y_chunks[i], chunks[i])\n",
    "    #     AP_window.append(score)\n",
    "\n",
    "    # OAP = average_precision_score(y_adjusted, all_scores) \n",
    "    # MAP = sum(AP_window)/len(AP_window)\n",
    "    # AUC = roc_auc_score(y_adjusted, all_scores)\n",
    "\n",
    "    # print(\"XStream: OAP =\", OAP,\"\\n\\t \",\n",
    "    #     \"MAP =\", MAP, \"\\n\\t \", \n",
    "    #     \"AUC =\", AUC)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uni_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "09134615d3c11d3d66d95b41ba5927bd89680aa8b55941e9aa91f4d36f92235d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
